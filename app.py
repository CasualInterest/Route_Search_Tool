import os
from pathlib import Path
import shutil
import numpy as np
import pandas as pd
import streamlit as st
import pydeck as pdk
from datetime import datetime
from supabase import create_client, Client

# =========================

# Config

# =========================

st.set_page_config(page_title=‚ÄúRoute Search Tool‚Äù, layout=‚Äúwide‚Äù)

# Supabase connection

def get_supabase_client() -> Client:
‚Äú‚Äù‚ÄúGet Supabase client from secrets‚Äù‚Äù‚Äù
try:
url = st.secrets[‚Äúsupabase‚Äù][‚Äúurl‚Äù]
key = st.secrets[‚Äúsupabase‚Äù][‚Äúkey‚Äù]
return create_client(url, key)
except Exception as e:
st.error(f‚ÄùSupabase configuration error: {e}‚Äù)
st.info(‚ÄúAdd ‚Äòsupabase‚Äô section to Streamlit Secrets with ‚Äòurl‚Äô and ‚Äòkey‚Äô‚Äù)
st.stop()

supabase = get_supabase_client()
TABLE_NAME = ‚Äúroutes‚Äù

IATA_LATLONG_CSV = os.environ.get(‚ÄúIATA_LATLONG_CSV‚Äù, ‚Äúiata_latlong.csv‚Äù)
BACKUP_DIR = Path(‚Äúbackups‚Äù)

DISPLAY_COLS = [‚ÄúDest‚Äù, ‚ÄúOrigin‚Äù, ‚ÄúFreq‚Äù, ‚ÄúA/L‚Äù, ‚ÄúEQPT‚Äù, ‚ÄúEff Date‚Äù, ‚ÄúTerm Date‚Äù]
FLEET_ALLOWED = [‚Äú220‚Äù, ‚Äú320‚Äù, ‚Äú737‚Äù, ‚Äú757/767‚Äù, ‚Äú764‚Äù, ‚Äú330‚Äù, ‚Äú350‚Äù, ‚Äú717‚Äù, ‚ÄúRJ‚Äù, ‚ÄúOther‚Äù]

SENSITIVE_COLS = [
‚ÄúFlight‚Äù, ‚ÄúFlight #‚Äù, ‚ÄúFlt#‚Äù, ‚ÄúFlt‚Äù, ‚ÄúFLT‚Äù, ‚ÄúFLIGHT‚Äù,
‚ÄúDep Time‚Äù, ‚ÄúDeparture Time‚Äù, ‚ÄúSTD‚Äù, ‚ÄúETD‚Äù, ‚ÄúDept Time‚Äù, ‚ÄúDEP TIME‚Äù,
]

# =========================

# Secrets / Passwords

# =========================

def _get_secret(key: str):
try:
return st.secrets[key]
except Exception:
return os.environ.get(key)

VIEW_PASSWORD = _get_secret(‚ÄúVIEW_PASSWORD‚Äù)
ADMIN_PASSWORD = _get_secret(‚ÄúADMIN_PASSWORD‚Äù)

# =========================

# Viewer Auth Gate

# =========================

if ‚Äúviewer_authenticated‚Äù not in st.session_state:
st.session_state[‚Äúviewer_authenticated‚Äù] = False

if not st.session_state[‚Äúviewer_authenticated‚Äù]:
st.title(‚ÄúRoute Search Tool‚Äù)
st.subheader(‚Äúüîí Enter Password to Continue‚Äù)

```
if not VIEW_PASSWORD:
    st.error("Viewer password not configured. Add VIEW_PASSWORD in Streamlit Secrets.")
    st.stop()

pw = st.text_input("Password", type="password")
if st.button("Login", type="primary"):
    if pw == VIEW_PASSWORD:
        st.session_state["viewer_authenticated"] = True
        st.rerun()
    else:
        st.error("Incorrect password.")
st.stop()
```

# =========================

# Hard reset helper

# =========================

def hard_reset():
‚Äú‚Äù‚ÄúFull nuke reset, only used by the manual Restart App button.‚Äù‚Äù‚Äù
try:
st.cache_data.clear()
except Exception:
pass
try:
st.cache_resource.clear()
except Exception:
pass
for _k in list(st.session_state.keys()):
try:
del st.session_state[_k]
except Exception:
pass
st.rerun()

# =========================

# Helpers

# =========================

def _to_na(s: pd.Series) -> pd.Series:
return (
s.astype(str)
.str.strip()
.replace({‚Äù‚Äù: np.nan, ‚Äúnan‚Äù: np.nan, ‚ÄúNaN‚Äù: np.nan, ‚ÄúNone‚Äù: np.nan})
)

def parse_any_date(series: pd.Series) -> pd.Series:
s = _to_na(series)
dt = pd.to_datetime(s, format=‚Äù%d%b%y‚Äù, errors=‚Äúcoerce‚Äù)
m = dt.isna()
if m.any():
dt.loc[m] = pd.to_datetime(
s.loc[m],
errors=‚Äúcoerce‚Äù,
dayfirst=False,
infer_datetime_format=True,
)
m = dt.isna()
if m.any():
as_num = pd.to_numeric(s.loc[m], errors=‚Äúcoerce‚Äù)
num_mask = as_num.notna()
if num_mask.any():
dt.loc[as_num.index[num_mask]] = pd.to_datetime(
as_num[num_mask],
unit=‚Äúd‚Äù,
origin=‚Äú1899-12-30‚Äù,
errors=‚Äúcoerce‚Äù,
)
return dt

def ensure_display_cols(df: pd.DataFrame) -> pd.DataFrame:
df = df.drop(columns=[c for c in df.columns if c in SENSITIVE_COLS], errors=‚Äúignore‚Äù)
for c in DISPLAY_COLS:
if c not in df.columns:
df[c] = pd.NA
return df[DISPLAY_COLS].copy()

def clean_origin(df: pd.DataFrame) -> pd.DataFrame:
if ‚ÄúOrigin‚Äù in df.columns:
df[‚ÄúOrigin‚Äù] = df[‚ÄúOrigin‚Äù].astype(str).str.strip()
df = df[~df[‚ÄúOrigin‚Äù].isin([‚Äù‚Äù, ‚Äúnan‚Äù, ‚ÄúNaN‚Äù, ‚ÄúNone‚Äù, ‚ÄúNONE‚Äù])]
return df

def map_to_fleet(eqpt: str) -> str:
‚Äú‚Äù‚ÄúMap EQPT code to fleet category‚Äù‚Äù‚Äù
if pd.isna(eqpt):
return ‚ÄúOther‚Äù
s = str(eqpt).strip().upper()

```
if "220" in s or "A220" in s:
    return "220"
if "32" in s or "A32" in s or "320" in s or "321" in s:
    return "320"
if "737" in s or "73" in s:
    return "737"
if "757" in s or "767" in s or "75" in s or "76" in s:
    return "757/767"
if "764" in s:
    return "764"
if "330" in s or "A330" in s or "33" in s:
    return "330"
if "350" in s or "A350" in s or "35" in s:
    return "350"
if "717" in s:
    return "717"
if "RJ" in s or "CRJ" in s or "ERJ" in s or "E17" in s or "E19" in s:
    return "RJ"
return "Other"
```

# =========================

# Supabase Data Functions

# =========================

@st.cache_data(ttl=60, show_spinner=‚ÄúLoading data from database‚Ä¶‚Äù)
def load_all_data_from_supabase() -> pd.DataFrame:
‚Äú‚Äù‚ÄúLoad all routes from Supabase‚Äù‚Äù‚Äù
try:
response = supabase.table(TABLE_NAME).select(‚Äù*‚Äù).execute()

```
    if not response.data:
        return pd.DataFrame(columns=DISPLAY_COLS)
    
    df = pd.DataFrame(response.data)
    
    if "Eff Date" in df.columns:
        df["Eff Date"] = parse_any_date(df["Eff Date"])
    if "Term Date" in df.columns:
        df["Term Date"] = parse_any_date(df["Term Date"])
    
    df = ensure_display_cols(df)
    df = clean_origin(df)
    
    return df

except Exception as e:
    st.error(f"Failed to load data from Supabase: {e}")
    return pd.DataFrame(columns=DISPLAY_COLS)
```

def upload_to_supabase(df: pd.DataFrame, batch_size: int = 1000) -> bool:
‚Äú‚Äù‚ÄúUpload DataFrame to Supabase in batches‚Äù‚Äù‚Äù
try:
df = ensure_display_cols(df)

```
    if "Eff Date" in df.columns:
        df["Eff Date"] = pd.to_datetime(df["Eff Date"], errors="coerce").dt.strftime("%Y-%m-%d")
    if "Term Date" in df.columns:
        df["Term Date"] = pd.to_datetime(df["Term Date"], errors="coerce").dt.strftime("%Y-%m-%d")
    
    df = df.where(pd.notna(df), None)
    
    total = len(df)
    for i in range(0, total, batch_size):
        batch = df.iloc[i:i+batch_size]
        records = batch.to_dict('records')
        supabase.table(TABLE_NAME).insert(records).execute()
    
    return True

except Exception as e:
    st.error(f"Upload to Supabase failed: {e}")
    return False
```

def merge_and_upsert_to_supabase(incoming_df: pd.DataFrame) -> tuple[int, int]:
‚Äú‚Äù‚ÄúMerge incoming data with existing data and upsert to Supabase‚Äù‚Äù‚Äù
try:
existing = load_all_data_from_supabase()
rows_before = len(existing)

```
    combined = pd.concat([existing, incoming_df], ignore_index=True)
    combined = combined.drop_duplicates(subset=DISPLAY_COLS, keep="last")
    combined = combined.sort_values(DISPLAY_COLS, kind="mergesort", ignore_index=True)
    
    rows_after = len(combined)
    
    supabase.table(TABLE_NAME).delete().neq("Dest", "").execute()
    success = upload_to_supabase(combined)
    
    if success:
        st.cache_data.clear()
        return rows_before, rows_after
    else:
        return rows_before, rows_before

except Exception as e:
    st.error(f"Merge failed: {e}")
    return 0, 0
```

def backup_to_csv() -> Path:
‚Äú‚Äù‚ÄúDownload current Supabase data to local CSV backup‚Äù‚Äù‚Äù
try:
BACKUP_DIR.mkdir(exist_ok=True)
timestamp = datetime.now().strftime(‚Äù%Y%m%d_%H%M%S‚Äù)
backup_path = BACKUP_DIR / f‚Äùbackup_{timestamp}.csv‚Äù

```
    df = load_all_data_from_supabase()
    df.to_csv(backup_path, index=False)
    
    return backup_path
except Exception as e:
    st.error(f"Backup failed: {e}")
    return None
```

def clear_all_data():
‚Äú‚Äù‚ÄúDelete all rows from Supabase table‚Äù‚Äù‚Äù
try:
supabase.table(TABLE_NAME).delete().neq(‚ÄúDest‚Äù, ‚Äú‚Äù).execute()
st.cache_data.clear()
return True
except Exception as e:
st.error(f‚ÄùClear failed: {e}‚Äù)
return False

def read_map_upload(file_like) -> pd.DataFrame:
name = getattr(file_like, ‚Äúname‚Äù, ‚Äú‚Äù).lower()

```
if name.endswith(".xlsx") or name.endswith(".xls"):
    df = pd.read_excel(file_like, header=0, skiprows=4, dtype=str, engine="openpyxl")
else:
    df = pd.read_csv(
        file_like,
        header=0,
        skiprows=4,
        dtype=str,
        encoding="utf-8",
        on_bad_lines="skip",
    )

df.columns = [str(c).strip().title() for c in df.columns]
df = df.drop(columns=[c for c in df.columns if c in SENSITIVE_COLS], errors="ignore")

rename_map = {
    "Sta": "Dest",
    "Dest (Sta)": "Dest",
    "Destination": "Dest",
    "Prev City": "Origin",
    "Prev  City": "Origin",
    "Prevcity": "Origin",
    "Eff Date": "Eff Date",
    "Term Date": "Term Date",
    "Freq": "Freq",
    "A/L": "A/L",
    "Eqpt": "EQPT",
}
df = df.rename(columns={c: rename_map.get(c, c) for c in df.columns})

if "Eff Date" in df.columns:
    df["Eff Date"] = parse_any_date(df["Eff Date"])
if "Term Date" in df.columns:
    df["Term Date"] = parse_any_date(df["Term Date"])

df = ensure_display_cols(df)
df = clean_origin(df)

return df
```

def clean_master_df(df: pd.DataFrame):
‚Äú‚Äù‚ÄúClean and validate data‚Äù‚Äù‚Äù
original_len = len(df)

```
df = ensure_display_cols(df)

if "Eff Date" in df.columns:
    df["Eff Date"] = parse_any_date(df["Eff Date"])
if "Term Date" in df.columns:
    df["Term Date"] = parse_any_date(df["Term Date"])

before_date_drop = len(df)
df = df.dropna(subset=["Eff Date", "Term Date"])
dropped_dates = before_date_drop - len(df)

before_origin_drop = len(df)
df = clean_origin(df)
dropped_origin = before_origin_drop - len(df)

total_dropped = original_len - len(df)

return df, total_dropped, dropped_dates, dropped_origin
```

# =========================

# Load Data

# =========================

data = load_all_data_from_supabase()

if data.empty:
st.warning(‚Äú‚ö†Ô∏è No data in database. Upload MAP files to get started.‚Äù)

# =========================

# TITLE AND MODE TOGGLE

# =========================

st.title(‚Äú‚úàÔ∏è Route Search Tool‚Äù)

mode = st.radio(
‚ÄúSelect Mode:‚Äù,
options=[‚Äúüîç Search Tool‚Äù, ‚Äúüó∫Ô∏è Fleet Destinations‚Äù],
horizontal=True,
label_visibility=‚Äúcollapsed‚Äù
)

# =========================

# MODE: SEARCH TOOL

# =========================

if mode == ‚Äúüîç Search Tool‚Äù:

```
# Viewer logout
with st.sidebar:
    if st.button("üö™ Logout (Viewer)", use_container_width=True):
        st.session_state["viewer_authenticated"] = False
        st.session_state["is_admin"] = False
        st.rerun()

# Sidebar - Filters
with st.sidebar:
    st.header("Filters")
    
    sel_date = st.date_input("Select Date", value=datetime.today())
    
    all_dests = sorted(data["Dest"].dropna().astype(str).unique()) if not data.empty else []
    all_origs = sorted(data["Origin"].dropna().astype(str).unique()) if not data.empty else []
    all_eqpts = sorted(data["EQPT"].dropna().astype(str).unique()) if not data.empty else []
    
    sel_dests = st.multiselect("Filter Dest (optional)", options=all_dests)
    sel_origs = st.multiselect("Filter Origin (optional)", options=all_origs)
    sel_fleets = st.multiselect("Filter Fleet (optional)", options=FLEET_ALLOWED)
    sel_eqpts = st.multiselect("Filter EQPT (optional)", options=all_eqpts)
    
    if st.button("Reset Filters", use_container_width=True):
        st.rerun()

# Admin Section
if "is_admin" not in st.session_state:
    st.session_state["is_admin"] = False

with st.sidebar:
    st.markdown("---")
    st.subheader("Admin")
    
    if not st.session_state["is_admin"]:
        admin_pw = st.text_input("Admin Password", type="password", key="admin_login")
        if st.button("Login Admin", use_container_width=True):
            if admin_pw == ADMIN_PASSWORD:
                st.session_state["is_admin"] = True
                st.rerun()
            else:
                st.error("Incorrect admin password.")
    else:
        st.success("‚úÖ Admin mode enabled")
        if st.button("Logout Admin", use_container_width=True):
            st.session_state["is_admin"] = False
            st.rerun()

# Admin Upload & Merge
if st.session_state.get("is_admin", False):
    st.sidebar.markdown("---")
    
    total_rows = len(data)
    st.sidebar.info(f"üíæ Database: {total_rows:,} rows")
    
    with st.sidebar.expander("‚ûï Upload & Merge MAP files", expanded=False):
        uploads = st.file_uploader(
            "Upload MAP files (.xlsx or .csv)",
            type=["xlsx", "csv", "xls"],
            accept_multiple_files=True,
            key="map_uploads",
        )
        
        if uploads:
            st.caption("Files to merge:")
            for u in uploads:
                st.write("‚Ä¢", u.name)
        
        if st.button("Process & Merge", type="primary", disabled=not uploads):
            parts, errors = [], []
            
            for up in uploads:
                try:
                    df_upload = read_map_upload(up)
                    parts.append(df_upload)
                except Exception as e:
                    errors.append(f"{getattr(up,'name','file')}: {e}")
            
            if errors:
                st.sidebar.error("Some files failed:\n" + "\n".join(errors))
            
            if parts:
                incoming = pd.concat(parts, ignore_index=True)
                cleaned, dropped_total, dropped_dates, dropped_origin = clean_master_df(incoming)
                
                if dropped_total > 0:
                    st.sidebar.warning(
                        f"Cleanup dropped {dropped_total} rows "
                        f"(invalid dates: {dropped_dates}, blank origin: {dropped_origin})."
                    )
                
                with st.spinner("Uploading to database..."):
                    rows_before, rows_after = merge_and_upsert_to_supabase(cleaned)
                    delta = rows_after - rows_before
                    
                    st.sidebar.success(
                        f"‚úÖ Merge complete!\n"
                        f"Rows: {rows_before:,} ‚Üí {rows_after:,} (Œî {delta:+,})"
                    )
                    st.rerun()
            else:
                st.sidebar.warning("No valid rows found to merge.")
    
    st.sidebar.markdown("---")
    st.sidebar.subheader("Maintenance")
    
    if st.sidebar.button("üíæ Download CSV Backup", use_container_width=True):
        backup_path = backup_to_csv()
        if backup_path:
            with open(backup_path, "rb") as f:
                st.sidebar.download_button(
                    "‚¨áÔ∏è Download Backup",
                    data=f,
                    file_name=f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv",
                    use_container_width=True
                )
    
    if st.sidebar.button("üßπ Clean & normalize database", use_container_width=True):
        with st.spinner("Cleaning database..."):
            raw_data = load_all_data_from_supabase()
            cleaned, dropped_total, dropped_dates, dropped_origin = clean_master_df(raw_data)
            
            clear_all_data()
            upload_to_supabase(cleaned)
            
            st.sidebar.success(
                f"Cleaned database. Dropped {dropped_total} rows "
                f"(invalid dates: {dropped_dates}, blank origin: {dropped_origin}). "
                f"Now {len(cleaned):,} rows."
            )
            st.cache_data.clear()
            st.rerun()
    
    _confirm_clear = st.sidebar.checkbox("Confirm delete all data")
    _btn_clear_all = st.sidebar.button("‚ö†Ô∏è Clear All Data")
    if _btn_clear_all and _confirm_clear:
        backup_to_csv()
        if clear_all_data():
            st.sidebar.success("All data cleared.")
            st.rerun()

# Filtering logic
df = data.copy()
sel_ts = pd.Timestamp(sel_date)

df["Fleet"] = df["EQPT"].apply(map_to_fleet)

if "Eff Date" in df.columns and "Term Date" in df.columns:
    df["Eff Date"] = parse_any_date(df["Eff Date"])
    df["Term Date"] = parse_any_date(df["Term Date"])
    mask_date = (
        (df["Eff Date"].notna())
        & (df["Term Date"].notna())
        & (df["Eff Date"] <= sel_ts)
        & (df["Term Date"] >= sel_ts)
    )
    df = df[mask_date]

    df["Eff Date"] = df["Eff Date"].dt.strftime("%Y-%m-%d")
    df["Term Date"] = df["Term Date"].dt.strftime("%Y-%m-%d")

if len(sel_dests) > 0:
    df = df[df["Dest"].astype(str).isin(sel_dests)]
if len(sel_origs) > 0:
    df = df[df["Origin"].astype(str).isin(sel_origs)]
if len(sel_fleets) > 0:
    df = df[df["Fleet"].isin(sel_fleets)]
if len(sel_eqpts) > 0:
    df = df[df["EQPT"].astype(str).isin(sel_eqpts)]

# Unique Destinations grid
def render_unique_dest_table(filtered_df: pd.DataFrame, n_cols: int = 7, height: int = 220):
    st.subheader("Unique Destinations")
    if filtered_df.empty:
        st.info("No data for current filters.")
        return []

    dest_col_candidates = [
        c for c in filtered_df.columns
        if c.strip().lower() in ("dest", "dest (sta)", "sta", "destination")
    ]
    dest_col = dest_col_candidates[0] if dest_col_candidates else "Dest"

    uniq = (
        filtered_df[dest_col]
        .astype(str)
        .str.strip()
        .replace({"nan": np.nan, "None": np.nan, "": np.nan})
        .dropna()
        .unique()
    )
    uniq = np.sort(uniq)
    if len(uniq) == 0:
        st.info("No unique destinations found.")
        return []

    rows = int(np.ceil(len(uniq) / n_cols))
    table = np.empty((rows, n_cols), dtype=object)
    table[:] = ""
    for i, val in enumerate(uniq):
        r = i // n_cols
        c = i % n_cols
        table[r, c] = val

    wide_df = pd.DataFrame(table, columns=[f"Dest {i+1}" for i in range(n_cols)])
    st.dataframe(wide_df, use_container_width=True, height=height)

    return list(uniq)

unique_list = render_unique_dest_table(df, n_cols=7, height=220)

# Results Table
st.subheader("Filtered Results")
st.write(f"Date: {sel_date} | Rows: {len(df):,}")

show_cols = [
    "Dest", "Origin",
    "A/L", "EQPT", "Fleet", "Eff Date", "Term Date"
]

for c in show_cols:
    if c not in df.columns:
        df[c] = pd.NA

st.dataframe(df[show_cols], use_container_width=True, height=420)
st.caption("Showing: Dest, Origin, A/L, EQPT, Fleet, Eff Date, Term Date")

# Map
@st.cache_data
def load_airports(path: str):
    if not Path(path).exists():
        return pd.DataFrame(columns=["Dest", "Lat", "Long"])
    a = pd.read_csv(path, dtype={"Dest": str, "Lat": float, "Long": float})
    a["Dest"] = a["Dest"].str.strip()
    return a

st.subheader("Map of Unique Destinations")

def render_map(unique_dests: list):
    if not unique_dests:
        st.info("No destinations to plot.")
        return

    airports_df = load_airports(IATA_LATLONG_CSV)

    points = (
        pd.DataFrame({"Dest": unique_dests})
        .merge(airports_df, on="Dest", how="left")
        .dropna(subset=["Lat", "Long"])
    )

    if points.empty:
        st.info("No coordinates available for current selection.")
        return

    deck = pdk.Deck(
        layers=[
            pdk.Layer(
                "ScatterplotLayer",
                data=points,
                get_position="[Long, Lat]",
                get_radius=80000,
                get_fill_color=[0, 120, 220, 160],
                pickable=True,
            )
        ],
        initial_view_state=pdk.ViewState(
            latitude=float(points["Lat"].mean()) if not points["Lat"].isna().all() else 39.0,
            longitude=float(points["Long"].mean()) if not points["Long"].isna().all() else -98.0,
            zoom=3,
            pitch=0,
            bearing=0,
        ),
        tooltip={"text": "Destination: {Dest}"},
        map_provider="mapbox",
        map_style=None,
    )
    st.pydeck_chart(deck, use_container_width=True)

render_map(unique_list)
```

# =========================

# MODE: FLEET DESTINATIONS

# =========================

else:

```
# Sidebar - Simple filters
with st.sidebar:
    if st.button("üö™ Logout (Viewer)", use_container_width=True):
        st.session_state["viewer_authenticated"] = False
        st.session_state["is_admin"] = False
        st.rerun()
    
    st.markdown("---")
    st.header("Fleet Destination Filters")
    
    # Get available months from data
    data_with_dates = data.copy()
    data_with_dates["Eff Date"] = parse_any_date(data_with_dates["Eff Date"])
    data_with_dates["Term Date"] = parse_any_date(data_with_dates["Term Date"])
    
    # Extract all year-month combinations
    all_months = set()
    
    if not data_with_dates.empty:
        for _, row in data_with_dates.iterrows():
            eff = row["Eff Date"]
            term = row["Term Date"]
            
            if pd.notna(eff) and pd.notna(term):
                current = eff
                while current <= term:
                    all_months.add(current.strftime("%Y-%m"))
                    if current.month == 12:
                        current = current.replace(year=current.year + 1, month=1)
                    else:
                        current = current.replace(month=current.month + 1)
    
    available_months = sorted(list(all_months), reverse=True)
    
    if not available_months:
        st.warning("No date data available.")
        selected_month = None
    else:
        # Month selector
        month_options = ["All"] + available_months
        selected_month_display = st.selectbox(
            "Select Month",
            options=month_options,
            format_func=lambda x: "All Months" if x == "All" else datetime.strptime(x, "%Y-%m").strftime("%B %Y")
        )
        selected_month = None if selected_month_display == "All" else selected_month_display
    
    # Fleet selector
    fleet_options = ["All"] + FLEET_ALLOWED
    selected_fleet = st.selectbox("Select Fleet", options=fleet_options)

# Main content area - Just the map
st.subheader("Fleet Destination Map")

# Filter data
filtered_data = data_with_dates.copy()
filtered_data["Fleet"] = filtered_data["EQPT"].apply(map_to_fleet)

# Apply month filter
if selected_month:
    year, month = map(int, selected_month.split("-"))
    month_start = datetime(year, month, 1)
    
    if month == 12:
        month_end = datetime(year + 1, 1, 1) - pd.Timedelta(days=1)
    else:
        month_end = datetime(year, month + 1, 1) - pd.Timedelta(days=1)
    
    filtered_data = filtered_data[
        (filtered_data["Eff Date"] <= month_end) &
        (filtered_data["Term Date"] >= month_start)
    ]

# Apply fleet filter
if selected_fleet != "All":
    filtered_data = filtered_data[filtered_data["Fleet"] == selected_fleet]

# Get unique destinations
fleet_dests = filtered_data["Dest"].dropna().astype(str).unique().tolist()

# Display info
fleet_text = selected_fleet if selected_fleet != "All" else "All Fleets"
if selected_month:
    month_text = datetime.strptime(selected_month, "%Y-%m").strftime("%B %Y")
    st.write(f"**{fleet_text}** flying to **{len(fleet_dests)} destinations** in **{month_text}**")
else:
    st.write(f"**{fleet_text}** flying to **{len(fleet_dests)} destinations** across all months")

# Render large map
@st.cache_data
def load_airports(path: str):
    if not Path(path).exists():
        return pd.DataFrame(columns=["Dest", "Lat", "Long"])
    a = pd.read_csv(path, dtype={"Dest": str, "Lat": float, "Long": float})
    a["Dest"] = a["Dest"].str.strip()
    return a

if not fleet_dests:
    st.info("No destinations found for selected filters.")
else:
    airports_df = load_airports(IATA_LATLONG_CSV)
    
    points = (
        pd.DataFrame({"Dest": fleet_dests})
        .merge(airports_df, on="Dest", how="left")
        .dropna(subset=["Lat", "Long"])
    )
    
    if points.empty:
        st.info("No coordinates available for destinations.")
    else:
        deck = pdk.Deck(
            layers=[
                pdk.Layer(
                    "ScatterplotLayer",
                    data=points,
                    get_position="[Long, Lat]",
                    get_radius=80000,
                    get_fill_color=[0, 120, 220, 160],
                    pickable=True,
                )
            ],
            initial_view_state=pdk.ViewState(
                latitude=float(points["Lat"].mean()) if not points["Lat"].isna().all() else 39.0,
                longitude=float(points["Long"].mean()) if not points["Long"].isna().all() else -98.0,
                zoom=3,
                pitch=0,
                bearing=0,
            ),
            tooltip={"text": "Destination: {Dest}"},
            map_provider="mapbox",
            map_style=None,
        )
        st.pydeck_chart(deck, use_container_width=True, height=600)
```
